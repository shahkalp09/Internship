{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7KFfeieD7z"
      },
      "source": [
        "# RWKV-v4neo Fine-Tuning\n",
        "\n",
        "[RWKV](https://github.com/BlinkDL/RWKV-LM) is an RNN with transformer-level performance\n",
        "\n",
        "\n",
        "This notebook aims to streamline fine-tuning RWKV-v4 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFIiAsrfvJy"
      },
      "source": [
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_qFjgYmtSfK",
        "outputId": "94d0fd12-91a5-4119-8dbe-20faa3894f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saving models to /content/drive/MyDrive/rwkv-v4neo-rnn-pile-tuning\n"
          ]
        }
      ],
      "source": [
        "#@title Google Drive Options { display-mode: \"form\" }\n",
        "save_models_to_drive = True #@param {type:\"boolean\"}\n",
        "drive_mount = '/content/drive' #@param {type:\"string\"}\n",
        "output_dir = 'rwkv-v4neo-rnn-pile-tuning' #@param {type:\"string\"}\n",
        "tuned_model_name = 'tuned-python' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "if save_models_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_mount)\n",
        "\n",
        "output_path = f\"{drive_mount}/MyDrive/{output_dir}\" if save_models_to_drive else f\"/content/{output_dir}\"\n",
        "os.makedirs(f\"{output_path}/{tuned_model_name}\", exist_ok=True)\n",
        "os.makedirs(f\"{output_path}/base_models/\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving models to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eivKJ6FP1_9z",
        "outputId": "6f38f4a0-1832-4686-ebe2-0c5cccfb946d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 13 06:21:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4lt0FTegJw9",
        "outputId": "f228a10c-0551-4f1b-da17-e288cc6002d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RWKV-LM'...\n",
            "remote: Enumerating objects: 1830, done.\u001b[K\n",
            "remote: Counting objects: 100% (825/825), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 1830 (delta 772), reused 685 (delta 670), pack-reused 1005\u001b[K\n",
            "Receiving objects: 100% (1830/1830), 15.81 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1160/1160), done.\n",
            "/content/RWKV-LM/RWKV-v4neo\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/blinkdl/RWKV-LM\n",
        "repo_dir = \"/content/RWKV-LM/RWKV-v4neo\"\n",
        "%cd $repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDavUrBsgKIV",
        "outputId": "f81a6d76-a37d-4038-bf20-72199f3f9cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.9\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed\n",
            "  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning==1.9)\n",
            "  Downloading torchmetrics-1.0.0-py3-none-any.whl (728 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.9) (4.7.1)\n",
            "Collecting lightning-utilities>=0.4.2 (from pytorch-lightning==1.9)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.11)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9) (3.8.4)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning==1.9) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.3.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning==1.9) (1.3.0)\n",
            "Building wheels for collected packages: deepspeed, pathtools\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844524 sha256=bea1f725272cd5fe858cd6835256b32931f0c08d0b18b04a4c065c060591016e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e80c4ef5dcb52ee20bdfb8eae569214011c89568ae40e0333f0504fc6aa439c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built deepspeed pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, ninja, hjson, smmap, setproctitle, sentry-sdk, lightning-utilities, docker-pycreds, huggingface-hub, gitdb, transformers, GitPython, wandb, torchmetrics, pytorch-lightning, deepspeed\n",
            "Successfully installed GitPython-3.1.32 deepspeed-0.9.5 docker-pycreds-0.4.0 gitdb-4.0.10 hjson-3.1.0 huggingface-hub-0.16.4 lightning-utilities-0.9.0 ninja-1.11.1 pathtools-0.1.2 pytorch-lightning-1.9.0 safetensors-0.3.1 sentry-sdk-1.28.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 torchmetrics-1.0.0 transformers-4.30.2 wandb-0.15.5\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers pytorch-lightning==1.9 deepspeed wandb ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt7y7vR6e6U3"
      },
      "source": [
        "## Load Base Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIgagN-Se3wi",
        "outputId": "f12dc050-8eb7-48a0-cb1d-fbbd4baac016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'rwkv-4-pile-169m'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Total 59 (delta 0), reused 0 (delta 0), pack-reused 59\u001b[K\n",
            "Unpacking objects: 100% (59/59), 6.26 KiB | 458.00 KiB/s, done.\n",
            "Using rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023.pth as base\n"
          ]
        }
      ],
      "source": [
        "#@title Base Model Options\n",
        "#@markdown Using any of the listed options will download the checkpoint from huggingface\n",
        "\n",
        "base_model_name = \"RWKV-4-Pile-169M\" #@param [\"RWKV-4-Pile-1B5\", \"RWKV-4-Pile-430M\", \"RWKV-4-Pile-169M\"]\n",
        "base_model_url = f\"https://huggingface.co/BlinkDL/{base_model_name.lower()}\"\n",
        "\n",
        "if base_model_name == \"RWKV-4-Pile-169M\":\n",
        "    n_layer = 12\n",
        "    n_embd = 768\n",
        "elif base_model_name == \"RWKV-4-Pile-430M\":\n",
        "    n_layer = 24\n",
        "    n_embd = 1024\n",
        "elif base_model_name == \"RWKV-4-Pile-1B5\":\n",
        "    n_layer = 24\n",
        "    n_embd = 2048\n",
        "\n",
        "!git lfs clone $base_model_url\n",
        "\n",
        "from glob import glob\n",
        "base_model_path = glob(f\"{base_model_name.lower()}/{base_model_name}*.pth\")[0]\n",
        "\n",
        "print(f\"Using {base_model_path} as base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOPnLelfJgP"
      },
      "source": [
        "## Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW5OmlXmvaIU",
        "outputId": "6be5918e-b28f-46a4-d1d5-6fb977d1b93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing /content/drive/MyDrive/table_train_3.txt (VERY slow. please wait)\n",
            "Raw length = 19417\n",
            "Tokenized length = 5955\n"
          ]
        }
      ],
      "source": [
        "#@title Training Data Options\n",
        "#@markdown `input_file` should be the path to a single file that contains the text you want to fine-tune with.\n",
        "#@markdown Either upload a file to this notebook instance or reference a file in your Google drive.\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=f'{repo_dir}/20B_tokenizer.json')\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/table_train_3.txt\" #@param {type:\"string\"}\n",
        "output_file = 'train.npy'\n",
        "\n",
        "print(f'Tokenizing {input_file} (VERY slow. please wait)')\n",
        "\n",
        "data_raw = open(input_file, encoding=\"utf-8\").read()\n",
        "print(f'Raw length = {len(data_raw)}')\n",
        "\n",
        "data_code = tokenizer.encode(data_raw)\n",
        "print(f'Tokenized length = {len(data_code)}')\n",
        "\n",
        "out = np.array(data_code, dtype='uint16')\n",
        "np.save(output_file, out, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4lz-3maeIwY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCw5_ASwMud",
        "outputId": "d7795e58-6d3a-415d-8cf5-37e093e4dd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########## work in progress ##########\n",
            "[2023-07-13 06:23:00,246] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "############################################################################\n",
            "#\n",
            "# RWKV-4 FP16 on 1x1 GPU, bsz 1x1x8=8, deepspeed_stage_2 \n",
            "#\n",
            "# Data = train.npy (numpy), ProjDir = rwkv-v4neo-rnn-pile-tuning\n",
            "#\n",
            "# Epoch = 0 to 99 (will continue afterwards), save every 1 epoch\n",
            "#\n",
            "# Each \"epoch\" = 1000 steps, 8000 samples, 3072000 tokens\n",
            "#\n",
            "# Model = 12 n_layer, 768 n_embd, 384 ctx_len\n",
            "#\n",
            "# Adam = lr 1e-05 to 1e-05, warmup 0 steps, beta (0.9, 0.999), eps 1e-08\n",
            "#\n",
            "# Found torch 2.0.1+cu118, recommend 1.13.1+cu117 or newer\n",
            "# Found deepspeed 0.9.5, recommend 0.7.0 (faster than newer versions)\n",
            "# Found pytorch_lightning 1.9.0, recommend 1.9.1 or newer\n",
            "#\n",
            "############################################################################\n",
            "\n",
            "{'load_model': 'rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023.pth', 'wandb': '', 'proj_dir': 'rwkv-v4neo-rnn-pile-tuning', 'random_seed': -1, 'data_file': 'train.npy', 'data_type': 'numpy', 'vocab_size': 50277, 'ctx_len': 384, 'epoch_steps': 1000, 'epoch_count': 100, 'epoch_begin': 0, 'epoch_save': 1, 'micro_bsz': 8, 'n_layer': 12, 'n_embd': 768, 'dim_att': 768, 'dim_ffn': 3072, 'pre_ffn': 0, 'head_qk': 0, 'tiny_att_dim': 0, 'tiny_att_layer': -999, 'lr_init': 1e-05, 'lr_final': 1e-05, 'warmup_steps': 0, 'beta1': 0.9, 'beta2': 0.999, 'adam_eps': 1e-08, 'grad_cp': 0, 'dropout': 0, 'weight_decay': 0, 'my_pile_version': 1, 'my_pile_stage': 0, 'my_pile_shift': -1, 'my_pile_edecay': 0, 'layerwise_lr': 1, 'ds_bucket_mb': 200, 'my_img_version': 0, 'my_img_size': 0, 'my_img_bit': 0, 'my_img_clip': 'x', 'my_img_clip_scale': 1, 'my_img_l1_scale': 0, 'my_img_encoder': 'x', 'my_sample_len': 0, 'my_ffn_shift': 1, 'my_att_shift': 1, 'my_pos_emb': 0, 'load_partial': 0, 'magic_prime': 0, 'my_qa_mask': 0, 'my_random_steps': 0, 'my_testing': '', 'my_exit': 99999999, 'my_exit_tokens': -1, 'logger': False, 'enable_checkpointing': False, 'default_root_dir': None, 'gradient_clip_val': 1.0, 'gradient_clip_algorithm': None, 'num_nodes': 1, 'num_processes': None, 'devices': '1', 'gpus': None, 'auto_select_gpus': None, 'tpu_cores': None, 'ipus': None, 'enable_progress_bar': True, 'overfit_batches': 0.0, 'track_grad_norm': -1, 'check_val_every_n_epoch': 100000000000000000000, 'fast_dev_run': False, 'accumulate_grad_batches': None, 'max_epochs': -1, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'val_check_interval': None, 'log_every_n_steps': 100000000000000000000, 'accelerator': 'gpu', 'strategy': 'deepspeed_stage_2', 'sync_batchnorm': False, 'precision': 'fp16', 'enable_model_summary': True, 'num_sanity_val_steps': 0, 'resume_from_checkpoint': None, 'profiler': None, 'benchmark': None, 'reload_dataloaders_every_n_epochs': 0, 'auto_lr_find': False, 'replace_sampler_ddp': False, 'detect_anomaly': False, 'auto_scale_batch_size': False, 'plugins': None, 'amp_backend': None, 'amp_level': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'inference_mode': True, 'my_timestamp': '2023-07-13-06-23-01', 'betas': (0.9, 0.999), 'real_bsz': 8, 'run_name': '50277 ctx384 L12 D768'}\n",
            "\n",
            "\n",
            "\n",
            "Note: you are using fp16 (might overflow). Try bf16 / tf32 for stable training.\n",
            "\n",
            "\n",
            "Current vocab size = 50277 (make sure it's correct)\n",
            "Data has 5955 tokens.\n",
            "RWKV_MY_TESTING \n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu118/wkv_384...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/wkv_384/build.ninja...\n",
            "Building extension module wkv_384...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_384 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=384 -std=c++17 -c /content/RWKV-LM/RWKV-v4neo/cuda/wkv_cuda.cu -o wkv_cuda.cuda.o \n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z15kernel_backwardIfEviiiPKT_S2_S2_S2_S2_S2_PS0_S3_S3_S3_\n",
            "    3072 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 52 registers, 448 bytes cmem[0], 16 bytes cmem[2]\n",
            "ptxas info    : Compiling entry function '_Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_' for 'sm_75'\n",
            "ptxas info    : Function properties for _Z14kernel_forwardIfEviiiPKT_S2_S2_S2_PS0_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 39 registers, 408 bytes cmem[0]\n",
            "[2/3] c++ -MMD -MF wkv_op.o.d -DTORCH_EXTENSION_NAME=wkv_384 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /content/RWKV-LM/RWKV-v4neo/cuda/wkv_op.cpp -o wkv_op.o \n",
            "[3/3] c++ wkv_op.o wkv_cuda.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_384.so\n",
            "Loading extension module wkv_384...\n",
            "########## Loading rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023.pth... ##########\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "50277 768   emb.weight\n",
            "768         blocks.0.ln1.weight\n",
            "768         blocks.0.ln1.bias\n",
            "768         blocks.0.ln2.weight\n",
            "768         blocks.0.ln2.bias\n",
            "768         blocks.0.ln0.weight\n",
            "768         blocks.0.ln0.bias\n",
            "768         blocks.0.att.time_decay\n",
            "768         blocks.0.att.time_first\n",
            "768         blocks.0.att.time_mix_k\n",
            "768         blocks.0.att.time_mix_v\n",
            "768         blocks.0.att.time_mix_r\n",
            "768   768   blocks.0.att.key.weight\n",
            "768   768   blocks.0.att.value.weight\n",
            "768   768   blocks.0.att.receptance.weight\n",
            "768   768   blocks.0.att.output.weight\n",
            "768         blocks.0.ffn.time_mix_k\n",
            "768         blocks.0.ffn.time_mix_r\n",
            "3072  768   blocks.0.ffn.key.weight\n",
            "768   768   blocks.0.ffn.receptance.weight\n",
            "768   3072  blocks.0.ffn.value.weight\n",
            "768         blocks.1.ln1.weight\n",
            "768         blocks.1.ln1.bias\n",
            "768         blocks.1.ln2.weight\n",
            "768         blocks.1.ln2.bias\n",
            "768         blocks.1.att.time_decay\n",
            "768         blocks.1.att.time_first\n",
            "768         blocks.1.att.time_mix_k\n",
            "768         blocks.1.att.time_mix_v\n",
            "768         blocks.1.att.time_mix_r\n",
            "768   768   blocks.1.att.key.weight\n",
            "768   768   blocks.1.att.value.weight\n",
            "768   768   blocks.1.att.receptance.weight\n",
            "768   768   blocks.1.att.output.weight\n",
            "768         blocks.1.ffn.time_mix_k\n",
            "768         blocks.1.ffn.time_mix_r\n",
            "3072  768   blocks.1.ffn.key.weight\n",
            "768   768   blocks.1.ffn.receptance.weight\n",
            "768   3072  blocks.1.ffn.value.weight\n",
            "768         blocks.2.ln1.weight\n",
            "768         blocks.2.ln1.bias\n",
            "768         blocks.2.ln2.weight\n",
            "768         blocks.2.ln2.bias\n",
            "768         blocks.2.att.time_decay\n",
            "768         blocks.2.att.time_first\n",
            "768         blocks.2.att.time_mix_k\n",
            "768         blocks.2.att.time_mix_v\n",
            "768         blocks.2.att.time_mix_r\n",
            "768   768   blocks.2.att.key.weight\n",
            "768   768   blocks.2.att.value.weight\n",
            "768   768   blocks.2.att.receptance.weight\n",
            "768   768   blocks.2.att.output.weight\n",
            "768         blocks.2.ffn.time_mix_k\n",
            "768         blocks.2.ffn.time_mix_r\n",
            "3072  768   blocks.2.ffn.key.weight\n",
            "768   768   blocks.2.ffn.receptance.weight\n",
            "768   3072  blocks.2.ffn.value.weight\n",
            "768         blocks.3.ln1.weight\n",
            "768         blocks.3.ln1.bias\n",
            "768         blocks.3.ln2.weight\n",
            "768         blocks.3.ln2.bias\n",
            "768         blocks.3.att.time_decay\n",
            "768         blocks.3.att.time_first\n",
            "768         blocks.3.att.time_mix_k\n",
            "768         blocks.3.att.time_mix_v\n",
            "768         blocks.3.att.time_mix_r\n",
            "768   768   blocks.3.att.key.weight\n",
            "768   768   blocks.3.att.value.weight\n",
            "768   768   blocks.3.att.receptance.weight\n",
            "768   768   blocks.3.att.output.weight\n",
            "768         blocks.3.ffn.time_mix_k\n",
            "768         blocks.3.ffn.time_mix_r\n",
            "3072  768   blocks.3.ffn.key.weight\n",
            "768   768   blocks.3.ffn.receptance.weight\n",
            "768   3072  blocks.3.ffn.value.weight\n",
            "768         blocks.4.ln1.weight\n",
            "768         blocks.4.ln1.bias\n",
            "768         blocks.4.ln2.weight\n",
            "768         blocks.4.ln2.bias\n",
            "768         blocks.4.att.time_decay\n",
            "768         blocks.4.att.time_first\n",
            "768         blocks.4.att.time_mix_k\n",
            "768         blocks.4.att.time_mix_v\n",
            "768         blocks.4.att.time_mix_r\n",
            "768   768   blocks.4.att.key.weight\n",
            "768   768   blocks.4.att.value.weight\n",
            "768   768   blocks.4.att.receptance.weight\n",
            "768   768   blocks.4.att.output.weight\n",
            "768         blocks.4.ffn.time_mix_k\n",
            "768         blocks.4.ffn.time_mix_r\n",
            "3072  768   blocks.4.ffn.key.weight\n",
            "768   768   blocks.4.ffn.receptance.weight\n",
            "768   3072  blocks.4.ffn.value.weight\n",
            "768         blocks.5.ln1.weight\n",
            "768         blocks.5.ln1.bias\n",
            "768         blocks.5.ln2.weight\n",
            "768         blocks.5.ln2.bias\n",
            "768         blocks.5.att.time_decay\n",
            "768         blocks.5.att.time_first\n",
            "768         blocks.5.att.time_mix_k\n",
            "768         blocks.5.att.time_mix_v\n",
            "768         blocks.5.att.time_mix_r\n",
            "768   768   blocks.5.att.key.weight\n",
            "768   768   blocks.5.att.value.weight\n",
            "768   768   blocks.5.att.receptance.weight\n",
            "768   768   blocks.5.att.output.weight\n",
            "768         blocks.5.ffn.time_mix_k\n",
            "768         blocks.5.ffn.time_mix_r\n",
            "3072  768   blocks.5.ffn.key.weight\n",
            "768   768   blocks.5.ffn.receptance.weight\n",
            "768   3072  blocks.5.ffn.value.weight\n",
            "768         blocks.6.ln1.weight\n",
            "768         blocks.6.ln1.bias\n",
            "768         blocks.6.ln2.weight\n",
            "768         blocks.6.ln2.bias\n",
            "768         blocks.6.att.time_decay\n",
            "768         blocks.6.att.time_first\n",
            "768         blocks.6.att.time_mix_k\n",
            "768         blocks.6.att.time_mix_v\n",
            "768         blocks.6.att.time_mix_r\n",
            "768   768   blocks.6.att.key.weight\n",
            "768   768   blocks.6.att.value.weight\n",
            "768   768   blocks.6.att.receptance.weight\n",
            "768   768   blocks.6.att.output.weight\n",
            "768         blocks.6.ffn.time_mix_k\n",
            "768         blocks.6.ffn.time_mix_r\n",
            "3072  768   blocks.6.ffn.key.weight\n",
            "768   768   blocks.6.ffn.receptance.weight\n",
            "768   3072  blocks.6.ffn.value.weight\n",
            "768         blocks.7.ln1.weight\n",
            "768         blocks.7.ln1.bias\n",
            "768         blocks.7.ln2.weight\n",
            "768         blocks.7.ln2.bias\n",
            "768         blocks.7.att.time_decay\n",
            "768         blocks.7.att.time_first\n",
            "768         blocks.7.att.time_mix_k\n",
            "768         blocks.7.att.time_mix_v\n",
            "768         blocks.7.att.time_mix_r\n",
            "768   768   blocks.7.att.key.weight\n",
            "768   768   blocks.7.att.value.weight\n",
            "768   768   blocks.7.att.receptance.weight\n",
            "768   768   blocks.7.att.output.weight\n",
            "768         blocks.7.ffn.time_mix_k\n",
            "768         blocks.7.ffn.time_mix_r\n",
            "3072  768   blocks.7.ffn.key.weight\n",
            "768   768   blocks.7.ffn.receptance.weight\n",
            "768   3072  blocks.7.ffn.value.weight\n",
            "768         blocks.8.ln1.weight\n",
            "768         blocks.8.ln1.bias\n",
            "768         blocks.8.ln2.weight\n",
            "768         blocks.8.ln2.bias\n",
            "768         blocks.8.att.time_decay\n",
            "768         blocks.8.att.time_first\n",
            "768         blocks.8.att.time_mix_k\n",
            "768         blocks.8.att.time_mix_v\n",
            "768         blocks.8.att.time_mix_r\n",
            "768   768   blocks.8.att.key.weight\n",
            "768   768   blocks.8.att.value.weight\n",
            "768   768   blocks.8.att.receptance.weight\n",
            "768   768   blocks.8.att.output.weight\n",
            "768         blocks.8.ffn.time_mix_k\n",
            "768         blocks.8.ffn.time_mix_r\n",
            "3072  768   blocks.8.ffn.key.weight\n",
            "768   768   blocks.8.ffn.receptance.weight\n",
            "768   3072  blocks.8.ffn.value.weight\n",
            "768         blocks.9.ln1.weight\n",
            "768         blocks.9.ln1.bias\n",
            "768         blocks.9.ln2.weight\n",
            "768         blocks.9.ln2.bias\n",
            "768         blocks.9.att.time_decay\n",
            "768         blocks.9.att.time_first\n",
            "768         blocks.9.att.time_mix_k\n",
            "768         blocks.9.att.time_mix_v\n",
            "768         blocks.9.att.time_mix_r\n",
            "768   768   blocks.9.att.key.weight\n",
            "768   768   blocks.9.att.value.weight\n",
            "768   768   blocks.9.att.receptance.weight\n",
            "768   768   blocks.9.att.output.weight\n",
            "768         blocks.9.ffn.time_mix_k\n",
            "768         blocks.9.ffn.time_mix_r\n",
            "3072  768   blocks.9.ffn.key.weight\n",
            "768   768   blocks.9.ffn.receptance.weight\n",
            "768   3072  blocks.9.ffn.value.weight\n",
            "768         blocks.10.ln1.weight\n",
            "768         blocks.10.ln1.bias\n",
            "768         blocks.10.ln2.weight\n",
            "768         blocks.10.ln2.bias\n",
            "768         blocks.10.att.time_decay\n",
            "768         blocks.10.att.time_first\n",
            "768         blocks.10.att.time_mix_k\n",
            "768         blocks.10.att.time_mix_v\n",
            "768         blocks.10.att.time_mix_r\n",
            "768   768   blocks.10.att.key.weight\n",
            "768   768   blocks.10.att.value.weight\n",
            "768   768   blocks.10.att.receptance.weight\n",
            "768   768   blocks.10.att.output.weight\n",
            "768         blocks.10.ffn.time_mix_k\n",
            "768         blocks.10.ffn.time_mix_r\n",
            "3072  768   blocks.10.ffn.key.weight\n",
            "768   768   blocks.10.ffn.receptance.weight\n",
            "768   3072  blocks.10.ffn.value.weight\n",
            "768         blocks.11.ln1.weight\n",
            "768         blocks.11.ln1.bias\n",
            "768         blocks.11.ln2.weight\n",
            "768         blocks.11.ln2.bias\n",
            "768         blocks.11.att.time_decay\n",
            "768         blocks.11.att.time_first\n",
            "768         blocks.11.att.time_mix_k\n",
            "768         blocks.11.att.time_mix_v\n",
            "768         blocks.11.att.time_mix_r\n",
            "768   768   blocks.11.att.key.weight\n",
            "768   768   blocks.11.att.value.weight\n",
            "768   768   blocks.11.att.receptance.weight\n",
            "768   768   blocks.11.att.output.weight\n",
            "768         blocks.11.ffn.time_mix_k\n",
            "768         blocks.11.ffn.time_mix_r\n",
            "3072  768   blocks.11.ffn.key.weight\n",
            "768   768   blocks.11.ffn.receptance.weight\n",
            "768   3072  blocks.11.ffn.value.weight\n",
            "768         ln_out.weight\n",
            "768         ln_out.bias\n",
            "50277 768   head.weight\n",
            "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "[2023-07-13 06:23:37,816] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "Enabling DeepSpeed FP16.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu118/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 59.61519956588745 seconds\n",
            "Rank: 0 partition count [1, 1, 1] and sizes[(169324032, False), (9216, False), (9216, False)] \n",
            "\n",
            "  | Name   | Type       | Params\n",
            "--------------------------------------\n",
            "0 | emb    | Embedding  | 38.6 M\n",
            "1 | blocks | ModuleList | 92.1 M\n",
            "2 | ln_out | LayerNorm  | 1.5 K \n",
            "3 | head   | Linear     | 38.6 M\n",
            "--------------------------------------\n",
            "169 M     Trainable params\n",
            "0         Non-trainable params\n",
            "169 M     Total params\n",
            "338.685   Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/1000 [00:00<?, ?it/s] \n",
            "{'zero_allow_untested_optimizer': True, 'zero_optimization': {'stage': 2, 'contiguous_gradients': True, 'overlap_comm': True, 'allgather_partitions': True, 'reduce_scatter': True, 'allgather_bucket_size': 200000000, 'reduce_bucket_size': 200000000, 'sub_group_size': 1000000000000}, 'activation_checkpointing': {'partition_activations': False, 'cpu_checkpointing': False, 'contiguous_memory_optimization': False, 'synchronize_checkpoint_boundary': False}, 'aio': {'block_size': 1048576, 'queue_depth': 8, 'single_submit': False, 'overlap_events': True, 'thread_count': 1}, 'gradient_accumulation_steps': 1, 'train_micro_batch_size_per_gpu': 8, 'gradient_clipping': 1.0, 'fp16': {'enabled': True, 'loss_scale': 0, 'initial_scale_power': 16, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}}\n",
            "\n",
            "Epoch 11:  18% 180/1000 [00:52<04:01,  3.40it/s, loss=0.0138, lr=1e-5, REAL it/s=3.440, Kt/s=10.60]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "Epoch 11:  18%|█▊        | 180/1000 [00:55<04:14,  3.22it/s, loss=0.0138, lr=1e-5, REAL it/s=3.440, Kt/s=10.60]\n"
          ]
        }
      ],
      "source": [
        "#@title Begin Training with these Options { display-mode: \"form\" }\n",
        "n_epoch = 100 #@param {type:\"integer\"}\n",
        "epoch_save_frequency = 1 #@param {type:\"integer\"}\n",
        "batch_size =  50 #@param {type:\"integer\"}\n",
        "ctx_len = 384 #@param {type:\"integer\"}\n",
        "precision = 'fp16' #@param ['fp16', 'bf16', 'bf32'] {type:\"string\"}\n",
        "\n",
        "epoch_save_path = f\"{output_path}/{tuned_model_name}\"\n",
        "\n",
        "\n",
        "!python train.py \\\n",
        "--load_model $base_model_path \\\n",
        "--wandb \"\" \\\n",
        "--proj_dir $output_dir \\\n",
        "--data_file  \"train.npy\" \\\n",
        "--data_type \"numpy\" \\\n",
        "--vocab_size 50277 \\\n",
        "--ctx_len $ctx_len \\\n",
        "--epoch_steps 1000 \\\n",
        "--epoch_count $n_epoch \\\n",
        "--epoch_begin 0 \\\n",
        "--epoch_save $epoch_save_frequency \\\n",
        "--micro_bsz 8 \\\n",
        "--n_layer $n_layer \\\n",
        "--n_embd $n_embd \\\n",
        "--pre_ffn 0 \\\n",
        "--head_qk 0 \\\n",
        "--lr_init 1e-5 \\\n",
        "--lr_final 1e-5 \\\n",
        "--warmup_steps 0 \\\n",
        "--beta1 0.9 \\\n",
        "--beta2 0.999 \\\n",
        "--adam_eps 1e-8 \\\n",
        "--accelerator gpu \\\n",
        "--devices 1 \\\n",
        "--precision $precision \\\n",
        "--strategy deepspeed_stage_2 \\\n",
        "--grad_cp 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_U8FeRSXykez",
        "outputId": "f13256d7-1719-4380-d8cc-4c8933a7cd04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/rwkv-10.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Source file path\n",
        "source_path = '/content/RWKV-LM/RWKV-v4neo/rwkv-v4neo-rnn-pile-tuning/rwkv-10.pth'\n",
        "\n",
        "# Destination folder path\n",
        "destination_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Transfer the file\n",
        "shutil.move(source_path, destination_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTutsmsycvyK",
        "outputId": "8040317c-0e8a-4fc8-9d8a-e3f5bf0a1958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZqjPZoIVk6I",
        "outputId": "98b32305-8bce-4c09-dbd7-44d3456767a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatRWKV'...\n",
            "remote: Enumerating objects: 1531, done.\u001b[K\n",
            "remote: Counting objects: 100% (512/512), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 1531 (delta 425), reused 372 (delta 342), pack-reused 1019\u001b[K\n",
            "Receiving objects: 100% (1531/1531), 28.71 MiB | 17.57 MiB/s, done.\n",
            "Resolving deltas: 100% (848/848), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BlinkDL/ChatRWKV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x02XgYDLVoj5",
        "outputId": "38ee9bb4-b4d2-4dce-8164-3121362b6e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using /content/drive/MyDrive/rwkv-10.pth as base\n"
          ]
        }
      ],
      "source": [
        "'''#@title Select/Download Model { display-mode: \"form\" }\n",
        "import urllib\n",
        "\n",
        "#@markdown Select the model you'd like to use:\n",
        "model_file = \"/content/drive/MyDrive/rwkv-10.pth\" #@param {type:\"string\"}\n",
        "#@markdown It will first search `model_dir` for `model_file`.\n",
        "#@markdown If it isn't valid path, it will attempt to download a `RWKV-v4-Raven` model from huggingface.\n",
        "#@markdown To see which options you have, take a look at the [repo](https://huggingface.co/BlinkDL/rwkv-4-raven/).\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown For example:\n",
        "#@markdown - RWKV-v4-Raven-14B-v11x: `RWKV-4-Raven-14B-v11x-Eng99%-Other1%-20230501-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-7B-v11x: `RWKV-4-Raven-7B-v11x-Eng99%-Other1%-20230429-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-3B-v11: `RWKV-4-Raven-3B-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - RWKV-v4-Raven-1B5-v11: `RWKV-4-Raven-1B5-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - Custom Model: `/rwkv-subdirectory/custom-rwkv.pth`\n",
        "\n",
        "model_path = f\"{model_dir_path}/{model_file}\"\n",
        "if not os.path.exists(model_path):\n",
        "    model_repo = f\"https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main\"\n",
        "    model_url = f\"{model_repo}/{urllib.parse.quote_plus(model_file)}\"\n",
        "    try:\n",
        "        print(f\"Downloading '{model_file}' from {model_url} this may take a while\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(f\"Using {model_path} as base\")\n",
        "    except Exception as e:\n",
        "        print(f\"Model '{model_file}' doesn't exist\")\n",
        "        raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")'''\n",
        "\n",
        "#@title Select/Download Model { display-mode: \"form\" }\n",
        "import urllib\n",
        "import os\n",
        "#@markdown Select the model you'd like to use:\n",
        "model_file = \"/content/drive/MyDrive/rwkv-10.pth\" #@param {type:\"string\"}\n",
        "#@markdown It will first search `model_dir` for `model_file`.\n",
        "#@markdown If it isn't valid path, it will attempt to download a `RWKV-v4-Raven` model from huggingface.\n",
        "#@markdown To see which options you have, take a look at the [repo](https://huggingface.co/BlinkDL/rwkv-4-raven/).\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown For example:\n",
        "#@markdown - RWKV-v4-Raven-14B-v11x: `RWKV-4-Raven-14B-v11x-Eng99%-Other1%-20230501-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-7B-v11x: `RWKV-4-Raven-7B-v11x-Eng99%-Other1%-20230429-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-3B-v11: `RWKV-4-Raven-3B-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - RWKV-v4-Raven-1B5-v11: `RWKV-4-Raven-1B5-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - Custom Model: `/rwkv-subdirectory/custom-rwkv.pth`\n",
        "\n",
        "'''model_path = f\"{model_dir_path}/{model_file}\"\n",
        "if not os.path.exists(model_path):\n",
        "    model_repo = f\"https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main\"\n",
        "    model_url = f\"{model_repo}/{urllib.parse.quote_plus(model_file)}\"\n",
        "    try:\n",
        "        print(f\"Downloading '{model_file}' from {model_url} this may take a while\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(f\"Using {model_path} as base\")\n",
        "    except Exception as e:\n",
        "        print(f\"Model '{model_file}' doesn't exist\")\n",
        "        raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")'''\n",
        "model_path = model_file\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Model '{model_file}' doesn't exist\")\n",
        "    raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-rjwoXFWQV3",
        "outputId": "c9517b39-b6a6-46db-cb5b-28f9b15a1547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatRWKV v4 https://github.com/BlinkDL/ChatRWKV\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py310_cu118/wkv_cuda...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/wkv_cuda/build.ninja...\n",
            "Building extension module wkv_cuda...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module wkv_cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model - /content/drive/MyDrive/rwkv-10.pth\n",
            "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
            "\n",
            "Loading /content/drive/MyDrive/rwkv-10.pth ...\n",
            "Strategy: (total 12+1=13 layers)\n",
            "* cuda [float16, float16], store 13 layers\n",
            "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 \n",
            "emb.weight                        f16      cpu  50277   768 \n",
            "blocks.0.ln1.weight               f16   cuda:0    768       \n",
            "blocks.0.ln1.bias                 f16   cuda:0    768       \n",
            "blocks.0.ln2.weight               f16   cuda:0    768       \n",
            "blocks.0.ln2.bias                 f16   cuda:0    768       \n",
            "blocks.0.att.time_decay           f32   cuda:0    768       \n",
            "blocks.0.att.time_first           f32   cuda:0    768       \n",
            "blocks.0.att.time_mix_k           f16   cuda:0    768       \n",
            "blocks.0.att.time_mix_v           f16   cuda:0    768       \n",
            "blocks.0.att.time_mix_r           f16   cuda:0    768       \n",
            "blocks.0.att.key.weight           f16   cuda:0    768   768 \n",
            "blocks.0.att.value.weight         f16   cuda:0    768   768 \n",
            "blocks.0.att.receptance.weight    f16   cuda:0    768   768 \n",
            "blocks.0.att.output.weight        f16   cuda:0    768   768 \n",
            "blocks.0.ffn.time_mix_k           f16   cuda:0    768       \n",
            "blocks.0.ffn.time_mix_r           f16   cuda:0    768       \n",
            "blocks.0.ffn.key.weight           f16   cuda:0    768  3072 \n",
            "blocks.0.ffn.receptance.weight    f16   cuda:0    768   768 \n",
            "blocks.0.ffn.value.weight         f16   cuda:0   3072   768 \n",
            "....................................................................................................................................................................................\n",
            "blocks.11.ln1.weight              f16   cuda:0    768       \n",
            "blocks.11.ln1.bias                f16   cuda:0    768       \n",
            "blocks.11.ln2.weight              f16   cuda:0    768       \n",
            "blocks.11.ln2.bias                f16   cuda:0    768       \n",
            "blocks.11.att.time_decay          f32   cuda:0    768       \n",
            "blocks.11.att.time_first          f32   cuda:0    768       \n",
            "blocks.11.att.time_mix_k          f16   cuda:0    768       \n",
            "blocks.11.att.time_mix_v          f16   cuda:0    768       \n",
            "blocks.11.att.time_mix_r          f16   cuda:0    768       \n",
            "blocks.11.att.key.weight          f16   cuda:0    768   768 \n",
            "blocks.11.att.value.weight        f16   cuda:0    768   768 \n",
            "blocks.11.att.receptance.weight   f16   cuda:0    768   768 \n",
            "blocks.11.att.output.weight       f16   cuda:0    768   768 \n",
            "blocks.11.ffn.time_mix_k          f16   cuda:0    768       \n",
            "blocks.11.ffn.time_mix_r          f16   cuda:0    768       \n",
            "blocks.11.ffn.key.weight          f16   cuda:0    768  3072 \n",
            "blocks.11.ffn.receptance.weight   f16   cuda:0    768   768 \n",
            "blocks.11.ffn.value.weight        f16   cuda:0   3072   768 \n",
            "ln_out.weight                     f16   cuda:0    768       \n",
            "ln_out.bias                       f16   cuda:0    768       \n",
            "head.weight                       f16   cuda:0    768 50277 \n"
          ]
        }
      ],
      "source": [
        "#@title Load Model {\"display-mode\": \"form\"}\n",
        "import os, copy, types, gc, sys\n",
        "sys.path.append('ChatRWKV/rwkv_pip_package/src')\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\n",
        "except:\n",
        "    pass\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
        "args = types.SimpleNamespace()\n",
        "\n",
        "print('ChatRWKV v4 https://github.com/BlinkDL/ChatRWKV')\n",
        "\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "strategy = 'cuda fp16' #@param {\"type\": \"string\"}\n",
        "\n",
        "#@markdown Strategy Examples:\n",
        "#@markdown - `cpu fp32`\n",
        "#@markdown - `cuda:0 fp16 -> cuda:1 fp16`\n",
        "#@markdown - `cuda fp16i8 *10 -> cuda fp16`\n",
        "#@markdown - `cuda fp16i8`\n",
        "#@markdown - `cuda fp16i8 -> cpu fp32 *10`\n",
        "#@markdown - `cuda fp16i8 *10+`\n",
        "\n",
        "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "CHAT_LEN_SHORT = 40\n",
        "CHAT_LEN_LONG = 150\n",
        "FREE_GEN_LEN = 256\n",
        "\n",
        "CHUNK_LEN = 256 # split input into chunks to save VRAM (shorter -> slower)\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "from rwkv.model import RWKV\n",
        "from rwkv.utils import PIPELINE\n",
        "\n",
        "print(f'Loading model - {model_path}')\n",
        "model = RWKV(model=model_path, strategy=strategy)\n",
        "pipeline = PIPELINE(model, \"ChatRWKV/v2/20B_tokenizer.json\")\n",
        "END_OF_TEXT = 0\n",
        "END_OF_LINE = 187\n",
        "END_OF_LINE_DOUBLE = 535\n",
        "# pipeline = PIPELINE(model, \"cl100k_base\")\n",
        "# END_OF_TEXT = 100257\n",
        "# END_OF_LINE = 198\n",
        "\n",
        "model_tokens = []\n",
        "model_state = None\n",
        "\n",
        "AVOID_REPEAT = '，：？！'\n",
        "AVOID_REPEAT_TOKENS = []\n",
        "for i in AVOID_REPEAT:\n",
        "    dd = pipeline.encode(i)\n",
        "    assert len(dd) == 1\n",
        "    AVOID_REPEAT_TOKENS += dd\n",
        "\n",
        "def run_rnn(tokens, newline_adj = 0):\n",
        "    global model_tokens, model_state\n",
        "\n",
        "    tokens = [int(x) for x in tokens]\n",
        "    model_tokens += tokens\n",
        "    # print(f'### model ###\\n{tokens}\\n[{pipeline.decode(model_tokens)}]')\n",
        "\n",
        "    while len(tokens) > 0:\n",
        "        out, model_state = model.forward(tokens[:CHUNK_LEN], model_state)\n",
        "        tokens = tokens[CHUNK_LEN:]\n",
        "\n",
        "    out[END_OF_LINE] += newline_adj # adjust \\n probability\n",
        "\n",
        "    if model_tokens[-1] in AVOID_REPEAT_TOKENS:\n",
        "        out[model_tokens[-1]] = -999999999\n",
        "    return out\n",
        "\n",
        "all_state = {}\n",
        "def save_all_stat(srv, name, last_out):\n",
        "    n = f'{name}_{srv}'\n",
        "    all_state[n] = {}\n",
        "    all_state[n]['out'] = last_out\n",
        "    all_state[n]['rnn'] = copy.deepcopy(model_state)\n",
        "    all_state[n]['token'] = copy.deepcopy(model_tokens)\n",
        "\n",
        "def load_all_stat(srv, name):\n",
        "    global model_tokens, model_state\n",
        "    n = f'{name}_{srv}'\n",
        "    model_state = copy.deepcopy(all_state[n]['rnn'])\n",
        "    model_tokens = copy.deepcopy(all_state[n]['token'])\n",
        "    return all_state[n]['out']\n",
        "\n",
        "# Model only saw '\\n\\n' as [187, 187] before, but the tokenizer outputs [535] for it at the end\n",
        "def fix_tokens(tokens):\n",
        "    if len(tokens) > 0 and tokens[-1] == END_OF_LINE_DOUBLE:\n",
        "        tokens = tokens[:-1] + [END_OF_LINE, END_OF_LINE]\n",
        "    return tokens\n",
        "\n",
        "#@title Inference Setup {\"display-mode\": \"form\"}\n",
        "#@markdown Inference properties:\n",
        "temp = 1.1 #@param {\"type\": \"number\"}\n",
        "top_p = 0.7 #@param {\"type\": \"number\"}\n",
        "presence_penalty = 0.2 #@param {\"type\": \"number\"}\n",
        "frequency_penalty = 0.2 #@param {\"type\": \"number\"}\n",
        "# Run inference\n",
        "from prompt_toolkit import prompt\n",
        "\n",
        "PROMPT_FILE = 'ChatRWKV/v2/prompt/default/English-2.py'\n",
        "\n",
        "def load_prompt(PROMPT_FILE):\n",
        "    variables = {}\n",
        "    with open(PROMPT_FILE, 'rb') as file:\n",
        "        exec(compile(file.read(), PROMPT_FILE, 'exec'), variables)\n",
        "    user, bot, interface, init_prompt = variables['user'], variables['bot'], variables['interface'], variables['init_prompt']\n",
        "    init_prompt = init_prompt.strip().split('\\n')\n",
        "    for c in range(len(init_prompt)):\n",
        "        init_prompt[c] = init_prompt[c].strip().strip('\\u3000').strip('\\r')\n",
        "    init_prompt = '\\n' + ('\\n'.join(init_prompt)).strip() + '\\n\\n'\n",
        "    return user, bot, interface, init_prompt\n",
        "\n",
        "user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n",
        "out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n",
        "save_all_stat('', 'chat_init', out)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "srv_list = ['dummy_server']\n",
        "for s in srv_list:\n",
        "    save_all_stat(s, 'chat', out)\n",
        "\n",
        "def reply_msg(msg):\n",
        "    print(f'{bot}{interface} {msg}\\n')\n",
        "\n",
        "def on_message(message):\n",
        "    global model_tokens, model_state, user, bot, interface, init_prompt\n",
        "\n",
        "    srv = 'dummy_server'\n",
        "\n",
        "    msg = message.replace('\\\\n','\\n').strip()\n",
        "\n",
        "    x_temp = temp\n",
        "    x_top_p = top_p\n",
        "    if (\"-temp=\" in msg):\n",
        "        x_temp = float(msg.split(\"-temp=\")[1].split(\" \")[0])\n",
        "        msg = msg.replace(\"-temp=\"+f'{x_temp:g}', \"\")\n",
        "        # print(f\"temp: {x_temp}\")\n",
        "    if (\"-top_p=\" in msg):\n",
        "        x_top_p = float(msg.split(\"-top_p=\")[1].split(\" \")[0])\n",
        "        msg = msg.replace(\"-top_p=\"+f'{x_top_p:g}', \"\")\n",
        "        # print(f\"top_p: {x_top_p}\")\n",
        "    if x_temp <= 0.2:\n",
        "        x_temp = 0.2\n",
        "    if x_temp >= 5:\n",
        "        x_temp = 5\n",
        "    if x_top_p <= 0:\n",
        "        x_top_p = 0\n",
        "    msg = msg.strip()\n",
        "\n",
        "    if msg == '+reset':\n",
        "        out = load_all_stat('', 'chat_init')\n",
        "        save_all_stat(srv, 'chat', out)\n",
        "        reply_msg(\"Chat reset.\")\n",
        "        return\n",
        "\n",
        "    # use '+prompt {path}' to load a new prompt\n",
        "    elif msg[:8].lower() == '+prompt ':\n",
        "        print(\"Loading prompt...\")\n",
        "        try:\n",
        "            PROMPT_FILE = msg[8:].strip()\n",
        "            user, bot, interface, init_prompt = load_prompt(PROMPT_FILE)\n",
        "            out = run_rnn(fix_tokens(pipeline.encode(init_prompt)))\n",
        "            save_all_stat(srv, 'chat', out)\n",
        "            print(\"Prompt set up.\")\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        except:\n",
        "            print(\"Path error.\")\n",
        "\n",
        "    elif msg[:5].lower() == '+gen ' or msg[:3].lower() == '+i ' or msg[:4].lower() == '+qa ' or msg[:4].lower() == '+qq ' or msg.lower() == '+++' or msg.lower() == '++':\n",
        "\n",
        "        if msg[:5].lower() == '+gen ':\n",
        "            new = '\\n' + msg[5:].strip()\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:3].lower() == '+i ':\n",
        "            msg = msg[3:].strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "            new = f'''\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "# Instruction:\n",
        "{msg}\n",
        "\n",
        "# Response:\n",
        "'''\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:4].lower() == '+qq ':\n",
        "            new = '\\nQ: ' + msg[4:].strip() + '\\nA:'\n",
        "            # print(f'### prompt ###\\n[{new}]')\n",
        "            model_state = None\n",
        "            model_tokens = []\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg[:4].lower() == '+qa ':\n",
        "            out = load_all_stat('', 'chat_init')\n",
        "\n",
        "            real_msg = msg[4:].strip()\n",
        "            new = f\"{user}{interface} {real_msg}\\n\\n{bot}{interface}\"\n",
        "            # print(f'### qa ###\\n[{new}]')\n",
        "\n",
        "            out = run_rnn(pipeline.encode(new))\n",
        "            save_all_stat(srv, 'gen_0', out)\n",
        "\n",
        "        elif msg.lower() == '+++':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'gen_1')\n",
        "                save_all_stat(srv, 'gen_0', out)\n",
        "            except:\n",
        "                return\n",
        "\n",
        "        elif msg.lower() == '++':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'gen_0')\n",
        "            except:\n",
        "                return\n",
        "\n",
        "        begin = len(model_tokens)\n",
        "        out_last = begin\n",
        "        occurrence = {}\n",
        "        for i in range(FREE_GEN_LEN+100):\n",
        "            for n in occurrence:\n",
        "                out[n] -= (presence_penalty + occurrence[n] * frequency_penalty)\n",
        "            token = pipeline.sample_logits(\n",
        "                out,\n",
        "                temperature=x_temp,\n",
        "                top_p=x_top_p,\n",
        "            )\n",
        "            if token == END_OF_TEXT:\n",
        "                break\n",
        "            if token not in occurrence:\n",
        "                occurrence[token] = 1\n",
        "            else:\n",
        "                occurrence[token] += 1\n",
        "\n",
        "            if msg[:4].lower() == '+qa ':# or msg[:4].lower() == '+qq ':\n",
        "                out = run_rnn([token], newline_adj=-2)\n",
        "            else:\n",
        "                out = run_rnn([token])\n",
        "\n",
        "            xxx = pipeline.decode(model_tokens[out_last:])\n",
        "            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n",
        "                print(xxx, end='', flush=True)\n",
        "                out_last = begin + i + 1\n",
        "                if i >= FREE_GEN_LEN:\n",
        "                    break\n",
        "        print('\\n')\n",
        "        # send_msg = pipeline.decode(model_tokens[begin:]).strip()\n",
        "        # print(f'### send ###\\n[{send_msg}]')\n",
        "        # reply_msg(send_msg)\n",
        "        save_all_stat(srv, 'gen_1', out)\n",
        "\n",
        "    else:\n",
        "        if msg.lower() == '+':\n",
        "            try:\n",
        "                out = load_all_stat(srv, 'chat_pre')\n",
        "            except:\n",
        "                return\n",
        "        else:\n",
        "            out = load_all_stat(srv, 'chat')\n",
        "            msg = msg.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')\n",
        "            new = f\"{user}{interface} {msg}\\n\\n{bot}{interface}\"\n",
        "            # print(f'### add ###\\n[{new}]')\n",
        "            out = run_rnn(pipeline.encode(new), newline_adj=-999999999)\n",
        "            save_all_stat(srv, 'chat_pre', out)\n",
        "\n",
        "        begin = len(model_tokens)\n",
        "        out_last = begin\n",
        "        print(f'{bot}{interface}', end='', flush=True)\n",
        "        occurrence = {}\n",
        "        for i in range(999):\n",
        "            if i <= 0:\n",
        "                newline_adj = -999999999\n",
        "            elif i <= CHAT_LEN_SHORT:\n",
        "                newline_adj = (i - CHAT_LEN_SHORT) / 10\n",
        "            elif i <= CHAT_LEN_LONG:\n",
        "                newline_adj = 0\n",
        "            else:\n",
        "                newline_adj = min(3, (i - CHAT_LEN_LONG) * 0.25) # MUST END THE GENERATION\n",
        "\n",
        "            for n in occurrence:\n",
        "                out[n] -= (presence_penalty + occurrence[n] * frequency_penalty)\n",
        "            token = pipeline.sample_logits(\n",
        "                out,\n",
        "                temperature=x_temp,\n",
        "                top_p=x_top_p,\n",
        "            )\n",
        "            # if token == END_OF_TEXT:\n",
        "            #     break\n",
        "            if token not in occurrence:\n",
        "                occurrence[token] = 1\n",
        "            else:\n",
        "                occurrence[token] += 1\n",
        "\n",
        "            out = run_rnn([token], newline_adj=newline_adj)\n",
        "            out[END_OF_TEXT] = -999999999  # disable <|endoftext|>\n",
        "\n",
        "            xxx = pipeline.decode(model_tokens[out_last:])\n",
        "            if '\\ufffd' not in xxx: # avoid utf-8 display issues\n",
        "                print(xxx, end='', flush=True)\n",
        "                out_last = begin + i + 1\n",
        "\n",
        "            send_msg = pipeline.decode(model_tokens[begin:])\n",
        "            if '\\n\\n' in send_msg:\n",
        "                send_msg = send_msg.strip()\n",
        "                break\n",
        "\n",
        "        save_all_stat(srv, 'chat', out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-3_oqY7Wj66",
        "outputId": "ab263e65-d5ab-4ae8-c63c-62cc50e2f9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bob: +gen serial number 25 26 27 <start>\"\n",
            "sno<stop>\n",
            "\"name Ava Benjamin Charlotte <start>\"name<stop>\n",
            "\"birthdate 1993 1996 1991 <start>\"birthdate<stop>\n",
            "\"job Designer Engineer Lawyer <start>\"job<stop>\n",
            "\"serial number 29 30 31 <start>\"sno<stop>\n",
            "\"name Emma James Noah <start>\"name<stop>\n",
            "\"birthdate 1995 1992 1997 <start>\"birthdate<stop>\n",
            "\"job Doctor Architect Engineer <start>\"job<stop>\n",
            "\"serial number 33 34 35 <start>\"sno<stop>\n",
            "\"name William Mia Ethan <start>\"name<stop>\n",
            "\"birthdate 1997 1994 1999 <start>\"birthdate<stop>\n",
            "\"job Lawyer Designer Engineer <start>\"job<stop>\n",
            "\"serial number 37 38 39 <start>\"sno<stop>\n",
            "\"name Sophia Noah Emma <start>\"name<stop>\n",
            "\"birthdate 1991 1996 1994 <start>\"birthdate<stop>\n",
            "\"job Designer Engineer Doctor <start>\"job<stop>\n",
            "\"serial number 41 42 43 <start>\"sno<stop>\n",
            "\"name Benjamin Isabella David <start>\"name<stop>\n",
            "\"birthdate 1988 1995\n",
            "\n",
            "Bob: +gen \"name Emma James Noah <start>\"\n",
            "name<stop>\n",
            "\"birthdate 1995 1992 1997 <start>\"birthdate<stop>\n",
            "\"job Doctor Architect Engineer <start>\"job<stop>\n",
            "\"serial number 73 74 75 <start>\"sno<stop>\n",
            "\"name William Mia Ethan <start>\"name<stop>\n",
            "\"birthdate 1997 1994 1999 <start>\"birthdate<stop>\n",
            "\"job Lawyer Designer Engineer <start>\"job<stop>\n",
            "\"serial number 77 78 79 <start>\"sno<stop>\n",
            "\"name Sophia Noah Emma <start>\"name<stop>\n",
            "\"birthdate 1991 1996 1994 <start>\"birthdate<stop>\n",
            "\"job Designer Engineer Doctor <start>\"job<stop>\n",
            "\"serial number 81 82 83 <start>\"sno<stop>\n",
            "\"name Benjamin Isabella David <start>\"name<stop>\n",
            "\"birthdate 1993 1995 1988 <start>\"birthdate<stop>\n",
            "\"job Engineer Designer Architect <start>\"job<stop>\n",
            "\"serial number 85 86 87 <start>\"sno<stop>\n",
            "\"name Ethan Olivia Emma <start>\"name<stop>\n",
            "\"birthdate 1993 1996 1994 <start>\"birthdate<stop>\n",
            "\"job Teacher Lawyer Doctor\n",
            "\n",
            "Bob: +gen 1995 1992 1999 <start>\"\n",
            "birthdate<stop>\n",
            "1989 Québec<stop>\n",
            "1990 Quebec<stop>\n",
            "1991 Quebec<stop>\n",
            "1992 Quebec<stop>\n",
            "1993 Quebec<stop>\n",
            "1994 Quebec<stop>\n",
            "1995 Quebec<stop>\n",
            "1996 Quebec<stop>\n",
            "1997 Quebec<stop>\n",
            "1998 Quebec<stop>\n",
            "1999 Quebec<stop>\n",
            "2000 Yukon<stop>\n",
            "2001 Yukon<stop>\n",
            "2002 Yukon<stop>\n",
            "2003 Yukon<stop>\n",
            "2003 Yukon<stop>\n",
            "2004 Yukon<stop>\n",
            "2005 Yukon<stop>\n",
            "2006 Yukon<stop>\n",
            "2007 Yukon<stop>\n",
            "2008 Yukon<stop>\n",
            "2009 Yukon<stop>\n",
            "2010 Yukon<stop>\n",
            "2011 Yukon<stop>\n",
            "2012 Yukon<stop>\n",
            "2013 Yukon<stop>\n",
            "2014 Yukon<stop>\n",
            "2015 Yukon<stop>\n",
            "2016 Yukon<stop>\n",
            "2017 Yukon<stop>\n",
            "2018 Yukon<stop>\n",
            "2019 Yukon<stop>\n",
            "2021 Yukon<start>\"birthdate<stop>\n",
            "2223 Yukon<start>\"birthdate<stop>\n",
            "2324 Yukon<start>\"birthdate<stop\n",
            "\n",
            "Bob: +gen job Doctor Architect Engineer <start>\"\n",
            "job<stop>\n",
            "\"serial number 349 350 351 <start>\"sno<stop>\n",
            "\"name Ava Benjamin Charlotte <start>\"name<stop>\n",
            "\"birthdate 1993 1996 1991 <start>\"birthdate<stop>\n",
            "\"job Designer Engineer Lawyer <start>\"job<stop>\n",
            "\""
          ]
        }
      ],
      "source": [
        "#@title Chat {\"display-mode\": \"form\"}\n",
        "\n",
        "#@markdown Running this cell will start the chat. Simply type your message in the input\n",
        "\n",
        "#@markdown Commands:\n",
        "#@markdown - `+` to get an alternate chat reply\n",
        "#@markdown - `+reset` to reset the chat\n",
        "#@markdown - `+gen YOUR PROMPT` for a free single-round generation with any prompt\n",
        "#@markdown - `+i YOUR INSTRUCT` for a free single-round generation with any instruct\n",
        "#@markdown - `+++` to continue the last free generation (only for `+gen` / `+i`)\n",
        "#@markdown - `++` to retry the last free generation (only for `+gen` / `+i`)\n",
        "\n",
        "#@markdown Remember to `+reset` periodically to clean up the bot's memory.\n",
        "\n",
        "while True:\n",
        "    msg = input(\"Bob: \")\n",
        "    if len(msg.strip()) > 0:\n",
        "        on_message(msg)\n",
        "    else:\n",
        "        print('Error: please say something')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdPo_kTiWqyK"
      },
      "outputs": [],
      "source": [
        "'''import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')'''\n",
        "!pip install langchain rwkv ninja\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNhh7_piYywd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/BlinkDL/RWKV-LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HYnpc30sDgk"
      },
      "outputs": [],
      "source": [
        "#@title Select/Download Model { display-mode: \"form\" }\n",
        "import urllib\n",
        "import os\n",
        "#@markdown Select the model you'd like to use:\n",
        "model_file = \"/content/drive/MyDrive/rwkv-12.pth\" #@param {type:\"string\"}\n",
        "#@markdown It will first search `model_dir` for `model_file`.\n",
        "#@markdown If it isn't valid path, it will attempt to download a `RWKV-v4-Raven` model from huggingface.\n",
        "#@markdown To see which options you have, take a look at the [repo](https://huggingface.co/BlinkDL/rwkv-4-raven/).\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown For example:\n",
        "#@markdown - RWKV-v4-Raven-14B-v11x: `RWKV-4-Raven-14B-v11x-Eng99%-Other1%-20230501-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-7B-v11x: `RWKV-4-Raven-7B-v11x-Eng99%-Other1%-20230429-ctx8192.pth`\n",
        "#@markdown - RWKV-v4-Raven-3B-v11: `RWKV-4-Raven-3B-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - RWKV-v4-Raven-1B5-v11: `RWKV-4-Raven-1B5-v11-Eng99%-Other1%-20230425-ctx4096.pth`\n",
        "#@markdown - Custom Model: `/rwkv-subdirectory/custom-rwkv.pth`\n",
        "\n",
        "'''model_path = f\"{model_dir_path}/{model_file}\"\n",
        "if not os.path.exists(model_path):\n",
        "    model_repo = f\"https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main\"\n",
        "    model_url = f\"{model_repo}/{urllib.parse.quote_plus(model_file)}\"\n",
        "    try:\n",
        "        print(f\"Downloading '{model_file}' from {model_url} this may take a while\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(f\"Using {model_path} as base\")\n",
        "    except Exception as e:\n",
        "        print(f\"Model '{model_file}' doesn't exist\")\n",
        "        raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")'''\n",
        "model_path = model_file\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Model '{model_file}' doesn't exist\")\n",
        "    raise Exception\n",
        "else:\n",
        "    print(f\"Using {model_path} as base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bDzlcB4sZwj",
        "outputId": "0952d01e-a418-4fbe-f38b-8860832db883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
            "\n",
            "Loading /content/drive/MyDrive/rwkv-11.pth ...\n",
            "Strategy: (total 12+1=13 layers)\n",
            "* cuda [float16, uint8], store 13 layers\n",
            "* cuda fp16, store 0 layers\n",
            "0-cuda-float16-uint8 1-cuda-float16-uint8 2-cuda-float16-uint8 3-cuda-float16-uint8 4-cuda-float16-uint8 5-cuda-float16-uint8 6-cuda-float16-uint8 7-cuda-float16-uint8 8-cuda-float16-uint8 9-cuda-float16-uint8 10-cuda-float16-uint8 11-cuda-float16-uint8 12-cuda-float16-uint8 \n",
            "emb.weight                        f16      cpu  50277   768 \n",
            "blocks.0.ln1.weight               f16   cuda:0    768       \n",
            "blocks.0.ln1.bias                 f16   cuda:0    768       \n",
            "blocks.0.ln2.weight               f16   cuda:0    768       \n",
            "blocks.0.ln2.bias                 f16   cuda:0    768       \n",
            "blocks.0.att.time_decay           f32   cuda:0    768       \n",
            "blocks.0.att.time_first           f32   cuda:0    768       \n",
            "blocks.0.att.time_mix_k           f16   cuda:0    768       \n",
            "blocks.0.att.time_mix_v           f16   cuda:0    768       \n",
            "blocks.0.att.time_mix_r           f16   cuda:0    768       \n",
            "blocks.0.att.key.weight            i8   cuda:0    768   768 \n",
            "blocks.0.att.value.weight          i8   cuda:0    768   768 \n",
            "blocks.0.att.receptance.weight     i8   cuda:0    768   768 \n",
            "blocks.0.att.output.weight         i8   cuda:0    768   768 \n",
            "blocks.0.ffn.time_mix_k           f16   cuda:0    768       \n",
            "blocks.0.ffn.time_mix_r           f16   cuda:0    768       \n",
            "blocks.0.ffn.key.weight            i8   cuda:0    768  3072 \n",
            "blocks.0.ffn.receptance.weight     i8   cuda:0    768   768 \n",
            "blocks.0.ffn.value.weight          i8   cuda:0   3072   768 \n",
            "....................................................................................................................................................................................\n",
            "blocks.11.ln1.weight              f16   cuda:0    768       \n",
            "blocks.11.ln1.bias                f16   cuda:0    768       \n",
            "blocks.11.ln2.weight              f16   cuda:0    768       \n",
            "blocks.11.ln2.bias                f16   cuda:0    768       \n",
            "blocks.11.att.time_decay          f32   cuda:0    768       \n",
            "blocks.11.att.time_first          f32   cuda:0    768       \n",
            "blocks.11.att.time_mix_k          f16   cuda:0    768       \n",
            "blocks.11.att.time_mix_v          f16   cuda:0    768       \n",
            "blocks.11.att.time_mix_r          f16   cuda:0    768       \n",
            "blocks.11.att.key.weight           i8   cuda:0    768   768 \n",
            "blocks.11.att.value.weight         i8   cuda:0    768   768 \n",
            "blocks.11.att.receptance.weight    i8   cuda:0    768   768 \n",
            "blocks.11.att.output.weight        i8   cuda:0    768   768 \n",
            "blocks.11.ffn.time_mix_k          f16   cuda:0    768       \n",
            "blocks.11.ffn.time_mix_r          f16   cuda:0    768       \n",
            "blocks.11.ffn.key.weight           i8   cuda:0    768  3072 \n",
            "blocks.11.ffn.receptance.weight    i8   cuda:0    768   768 \n",
            "blocks.11.ffn.value.weight         i8   cuda:0   3072   768 \n",
            "ln_out.weight                     f16   cuda:0    768       \n",
            "ln_out.bias                       f16   cuda:0    768       \n",
            "head.weight                        i8   cuda:0    768 50277 \n"
          ]
        }
      ],
      "source": [
        "#@title Load Model\n",
        "import os\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
        "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
        "\n",
        "from langchain.llms import RWKV\n",
        "\n",
        "strategy = \"cuda fp16i8 *20 -> cuda fp16\" #@param {\"type\":\"string\"}\n",
        "model = RWKV(model=model_path, strategy=strategy, tokens_path=\"RWKV-LM/RWKV-v4/20B_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPL3Xcvfskul",
        "outputId": "c874b03f-175c-47a8-d216-bda9e8ee67d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The response\n",
            "\n",
            "# Function to calculate the sum of values in a column\n",
            "#\n",
            "# Function to compare the values of two columns\n",
            "#\n",
            "# Function to compare the values of two columns\n",
            "#\n",
            "# Function to compare the values of two columns\n",
            "#\n",
            "# Function to compare the values of two columns\n",
            "#\n",
            "# Function to compare the values of two columns\n",
            "#\n",
            "def get_sum_of_two_columns(df, column1, column2, new_column_name):\n",
            "    df[new_column_name] = df[column1] + df[column2]\n",
            "    return df\n",
            "\n",
            "# Function to calculate the mean of a column\n",
            "def calculate_mean_of_column(df, column_name):\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Chain\n",
        "#@markdown A simple chain example. You first create the instruction template, and feed in your prompt as the instruction variable.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "task = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "# Instruction:\n",
        "{instruction}\n",
        "\n",
        "# Response:\n",
        "\"\"\"\n",
        "instruction = \"Function to get the count of unique values in a column\" #@param {type:\"string\"}\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"instruction\"],\n",
        "    template=task,\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=model, prompt=prompt)\n",
        "\n",
        "print(chain.run(instruction))\n",
        "\n",
        "#@markdown Documentation —\n",
        "#@markdown [PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/prompt_serialization.html),\n",
        "#@markdown [LLMChain](https://python.langchain.com/en/latest/modules/chains/generic/llm_chain.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a9Qxof7s8rj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}